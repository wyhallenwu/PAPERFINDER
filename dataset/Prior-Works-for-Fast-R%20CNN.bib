@article{abd1c342495432171beb7ca8fd9551ef13cbd0ff,
title = {ImageNet classification with deep convolutional neural networks},
year = {2012},
url = {https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {A. Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
journal = {Communications of the ACM},
volume = {60},
pages = {84 - 90},
doi = {10.1145/3065386},
}

@article{2f4df08d9072fc2ac181b7fced6a245315ce05c8,
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
year = {2013},
url = {https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8},
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
author = {Ross B. Girshick and Jeff Donahue and Trevor Darrell and J. Malik},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
volume = {},
pages = {580-587},
doi = {10.1109/CVPR.2014.81},
arxivid = {1311.2524},
}

@article{82635fb63640ae95f90ee9bdc07832eb461ca881,
title = {The Pascal Visual Object Classes (VOC) Challenge},
year = {2010},
url = {https://www.semanticscholar.org/paper/82635fb63640ae95f90ee9bdc07832eb461ca881},
abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
author = {M. Everingham and L. Gool and Christopher K. I. Williams and J. Winn and Andrew Zisserman},
journal = {International Journal of Computer Vision},
volume = {88},
pages = {303-338},
doi = {10.1007/s11263-009-0275-4},
}

@article{1109b663453e78a59e4f66446d71720ac58cec25,
title = {OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks},
year = {2014},
url = {https://semanticscholar.org/paper/1109b663453e78a59e4f66446d71720ac58cec25},
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
author = {Pierre  Sermanet and David  Eigen and Xiang  Zhang and Michaël  Mathieu and Rob  Fergus and Yann  LeCun},
journal = {CoRR},
volume = {abs/1312.6229},
pages = {null},
arxivid = {1312.6229},
}

@article{38b6540ddd5beebffd05047c78183f7575559fb2,
title = {Selective Search for Object Recognition},
year = {2013},
url = {https://www.semanticscholar.org/paper/38b6540ddd5beebffd05047c78183f7575559fb2},
abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html).},
author = {J. Uijlings and K. V. D. Sande and T. Gevers and A. Smeulders},
journal = {International Journal of Computer Vision},
volume = {104},
pages = {154-171},
doi = {10.1007/s11263-013-0620-5},
}

@article{cbb19236820a96038d000dc629225d36e0b6294a,
title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
year = {2014},
url = {https://www.semanticscholar.org/paper/cbb19236820a96038d000dc629225d36e0b6294a},
abstract = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224<inline-formula><tex-math>$\times$ </tex-math><alternatives><inline-graphic xlink:type="simple" xlink:href="he-ieq1-2389824.gif"/></alternatives></inline-formula>224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 <inline-formula><tex-math>$\times$</tex-math><alternatives><inline-graphic xlink:type="simple" xlink:href="he-ieq2-2389824.gif"/> </alternatives></inline-formula> faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
volume = {37},
pages = {1904-1916},
doi = {10.1007/978-3-319-10578-9_23},
arxivid = {1406.4729},
}

@article{a8e8f3c8d4418c8d62e306538c9c1292635e9d27,
title = {Backpropagation Applied to Handwritten Zip Code Recognition},
year = {1989},
url = {https://www.semanticscholar.org/paper/a8e8f3c8d4418c8d62e306538c9c1292635e9d27},
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {Yann LeCun and B. Boser and J. Denker and D. Henderson and R. Howard and W. Hubbard and L. Jackel},
journal = {Neural Computation},
volume = {1},
pages = {541-551},
doi = {10.1162/neco.1989.1.4.541},
}

@article{eb42cf88027de515750f230b23b1a057dc782108,
title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
year = {2014},
url = {https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108},
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
author = {K. Simonyan and Andrew Zisserman},
journal = {CoRR},
volume = {abs/1409.1556},
pages = {},
arxivid = {1409.1556},
}

@article{7ffdbc358b63378f07311e883dddacc9faeeaf4b,
title = {Fast R-CNN},
year = {2015},
url = {https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b},
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
author = {Ross B. Girshick},
doi = {10.1109/ICCV.2015.169},
arxivid = {1504.08083},
}

@article{71b7178df5d2b112d07e45038cb5637208659ff7,
title = {Microsoft COCO: Common Objects in Context},
year = {2014},
url = {https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7},
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
author = {Tsung-Yi Lin and M. Maire and Serge J. Belongie and James Hays and P. Perona and D. Ramanan and Piotr Dollár and C. L. Zitnick},
doi = {10.1007/978-3-319-10602-1_48},
arxivid = {1405.0312},
}
