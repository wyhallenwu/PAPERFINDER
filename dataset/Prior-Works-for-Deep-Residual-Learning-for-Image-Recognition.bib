@article{eb42cf88027de515750f230b23b1a057dc782108,
title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
year = {2014},
url = {https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108},
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
author = {K. Simonyan and Andrew Zisserman},
journal = {CoRR},
volume = {abs/1409.1556},
pages = {},
arxivid = {1409.1556},
}

@article{e15cf50aa89fee8535703b9f9512fca5bfc43327,
title = {Going deeper with convolutions},
year = {2014},
url = {https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327},
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Christian Szegedy and W. Liu and Y. Jia and Pierre Sermanet and Scott E. Reed and Dragomir Anguelov and D. Erhan and Vincent Vanhoucke and Andrew Rabinovich},
journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
volume = {},
pages = {1-9},
doi = {10.1109/CVPR.2015.7298594},
arxivid = {1409.4842},
}

@article{abd1c342495432171beb7ca8fd9551ef13cbd0ff,
title = {ImageNet classification with deep convolutional neural networks},
year = {2012},
url = {https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {A. Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
journal = {Communications of the ACM},
volume = {60},
pages = {84 - 90},
doi = {10.1145/3065386},
}

@article{e74f9b7f8eec6ba4704c206b93bc8079af3da4bd,
title = {ImageNet Large Scale Visual Recognition Challenge},
year = {2014},
url = {https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd},
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
author = {Olga Russakovsky and J. Deng and Hao Su and J. Krause and S. Satheesh and S. Ma and Zhiheng Huang and A. Karpathy and A. Khosla and Michael S. Bernstein and A. Berg and Li Fei-Fei},
journal = {International Journal of Computer Vision},
volume = {115},
pages = {211-252},
doi = {10.1007/s11263-015-0816-y},
arxivid = {1409.0575},
}

@article{2f4df08d9072fc2ac181b7fced6a245315ce05c8,
title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
year = {2013},
url = {https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8},
abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.},
author = {Ross B. Girshick and Jeff Donahue and Trevor Darrell and J. Malik},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
volume = {},
pages = {580-587},
doi = {10.1109/CVPR.2014.81},
arxivid = {1311.2524},
}

@article{d6f2f611da110b5b5061731be3fc4c7f45d8ee23,
title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
year = {2015},
url = {https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23},
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.},
author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
journal = {2015 IEEE International Conference on Computer Vision (ICCV)},
volume = {},
pages = {1026-1034},
doi = {10.1109/ICCV.2015.123},
arxivid = {1502.01852},
}

@article{2c03df8b48bf3fa39054345bafabfeff15bfd11d,
title = {Deep Residual Learning for Image Recognition},
year = {2015},
url = {https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d},
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
author = {Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
doi = {10.1109/cvpr.2016.90},
arxivid = {1512.03385},
}

@article{4d376d6978dad0374edfa6709c9556b42d3594d3,
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
url = {https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
author = {S. Ioffe and Christian Szegedy},
journal = {ArXiv},
volume = {abs/1502.03167},
pages = {},
arxivid = {1502.03167},
}

@article{424561d8585ff8ebce7d5d07de8dbf7aae5e7270,
title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
year = {2015},
url = {https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270},
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available},
author = {Shaoqing Ren and Kaiming He and Ross B. Girshick and J. Sun},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
volume = {39},
pages = {1137-1149},
doi = {10.1109/TPAMI.2016.2577031},
pmid = {27295650},
arxivid = {1506.01497},
}

@article{1109b663453e78a59e4f66446d71720ac58cec25,
title = {OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks},
year = {2013},
url = {https://www.semanticscholar.org/paper/1109b663453e78a59e4f66446d71720ac58cec25},
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
author = {Pierre Sermanet and D. Eigen and X. Zhang and Michaël Mathieu and R. Fergus and Yann LeCun},
journal = {CoRR},
volume = {abs/1312.6229},
pages = {},
arxivid = {1312.6229},
}
